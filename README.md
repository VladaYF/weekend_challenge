
## Инструкция по запуску:  
1.Склонировать репозиторий  
2.Запустить команду

```
docker-compose up --build
```
3.Открыть http://localhost:8080  
Параметры авторизации:  
- System - PostgreSQL  
- Username - postgres  
- Password - postgres  
- Database - postgres  
Результат: Созданы таблицы. В orders 1 записью.  

4.Перейти в "SQL command"  
5.Проверить заруженные данные в таблицах:  
```
    select *
    from orders;
```

## Что сделано:  

  1. Подготовка документа “Архитектура решения” - 12.04  
    1.1 Описать требования заказчика  
    1.2.Описать пример желаемого результата  
    1.3 Схема с технической архитектурой (MVP)  
    1.4 Технические компоненты  
    1.5 Допущения и ограничения  
    1.6 Список тестов для валидации  
    1.8 Список планируемых улучшений для дальнейшей масштабируемости  
    1.9 Приложение_1 (схемы слоев)  
    
  2. Проектирование схемы raw и Data Vault. Приложение_1 к ”Архитектура решения” -  12.04.  
    2.1 Определение бизнес сущностей   
    2.1.1 Создание модели данных для raw слоя   
    2.2 Разграничение сущностей по группам (Hab, Link, Satellit)   
    2.3 Проектирование модели данных Data Vault с учетом SCD Type 2 (добавление технических атрибутов для версионирования)   

  3. Подготовака в Docker контейнеров, docker-compose.yml - 13.04  
    3.1 Подготовка контейнера с БД - postgres.  
    3.2 Отладка запуска контейнера с init.sql с DDL для подготовки таблиц (raw схема + Data Vault).   
    3.3 Подготовка контейнера с Python - ETL.  
    5.4 Тестирование доступа к postgres из ETL.  
    3.5 Тестирование запуска и совместной работы контейнеров.   
    3.6 Описание инструкции по запуску.  

  5. Реализация core слоя проекта (кода с обработкой данных). Отладка локально - 14.04    
     4.1 DDL код для создания таблиц в БД.  
     4.2 Схема работы python кода. Алгоритм взаимодействия блоков (микросервисов) приложения.      


## Беклог:  

1. Реализация core слоя проекта (кода с обработкой данных). Отладка локально - 15.04  
    1.2 Создание синтетических данных (csv файлы) для обработки.   
    1.2 Python код ETL.   
    1.3 Python код с тестами для валидации данных.   

2. Заполнение README файла с подробным описанием компонентов core слоя - 15.04  
    2.1 Прописать компоненты, не выполненные в связи с ограничением времени.   
    2.2 Инструкция с навигацией по проекту.   
    2.2 Указать затраченное время на каждый из этапов и планируемые часы на завершение.   

3. Документация по запуску проекта - 15.04.

## Алгоритм ELT:  
Логика EL:  
0. Json с названием таблиц в raw слое и в data vault. 
1. Из таблицы логов получить day_number (1 если таблица пустая).
2. Коннект к БД. 
3. Шаблон пути для csv с номером дня и итерацией по списку таблиц.
4. Извлечь данные из csv по шаблону (только текущий день, предыдущие и будущие не обрабатываем). 
5. truncate бизнес-таблиц в raw слое. Загрузить бизнес данные в таблицы. 
6. Загрузить мету в таблицу с логами. 
7. *Получить схему raw таблиц. 
8. *Проверить, что схема таблиц и csv совпадает. Иначе выдать предупреждение.

Логика Transform:
1. Коннект к БД. 
2. Шаблон df pandas для vault схемы из Json
3. Получить данные из raw слоя
4. Df -  hub
5. Df - links
6. Df - satellite + колонка hash_diff (изменяемые атрибуты)
7. Открыть транзакцию.
8. Загрузить hub
	- инсерт всего что есть (поймать и пропустить ошибку ERROR: duplicate key value)
9. Загрузить links
	- инсерт всего что есть (поймать и пропустить ошибку ERROR: duplicate key value)
10. Загрузить satellit. 
	- инсерт всего что есть. Если id дублируется,то сравнить hash_diff в df и из таблицы. Если одинаковые - пропустить. 
		- если pазные - заполнить end_date из значения df load_date, и вставить новую строку. 
11. Закрыть транзакцию.

